---
title: "Test run time and memory usage of imputation accuracy"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: "Stefan McKinnon Hoj-Edwards"
output: 
  html_document: 
    theme: spacelab
    toc: yes
    toc_float: true
---


This vignette is for testing memory and running time of Siccuracy's `imputation_accuracy` function using native `gfortran` compiled `.so` and `ifort` compiled `.so`.
It repeats the previous memory test as it runs `imputation_accuracy` under both
`fast=TRUE` and `fast=FALSE`. When `fast=TRUE`, neither matrix is stored in memory, whereas when `fast=FALSE`,
the true genotype file is stored in memory for matching row IDs.

Further more, using `standardization=TRUE` forces the methods to read the true genotype file *twice* in order to calculate means and standard deviations of true genotypes.

To acomplish this, we do *not* load the `Siccuracy`-package.

In order to estimate memory usage, the calculation is launched in a subprocess, while using Gregor Gorjanc's [cpumemlog](https://github.com/gregorgorjanc/cpumemlog) to monitor the usage of the subprocess.

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(RSQLite)

BASEDIR <- getwd()
TMPDIR <- Sys.getenv('TMPDIR',tempdir())
TASKID <- Sys.getenv('SGE_TASK_ID', 'undefined')
suppressWarnings(TASKID <- as.integer(TASKID))
if (is.na(TASKID)) TASKID <- 0
SGE_JOBID <- Sys.getenv('JOB_ID', NA)


.dot <- function(x) {
  f <- function(..., relative=FALSE) {
    if (relative) {
      file.path(file.path(x, .Platform$file.sep, c(...)))
    } else {
      file.path(BASEDIR, x,  c(...))
    }
  } 
  null <- dir.create(f(''), FALSE, TRUE)
  return(f)
}
.cached <- .dot('cache_gfortran_vs_ifort')
.results <- .dot('gfortran_vs_ifort_results')

.do.computations = .Platform$OS.type != 'windows'

knitr::opts_knit$set(root.dir=TMPDIR)
knitr::opts_chunk$set(echo = TRUE, cache=FALSE, autodep=TRUE, 
                      cache.path = .cached('/'), 
                      fig.path = .results('', relative=TRUE), 
                      fig.width=10, fig.height=7)

system2 <- function(command, ..., stdout='', stderr='') {
  if (is.null(knitr::opts_current$get('results'))) 
    return(base::system2(command, ..., stdout=stdout, stderr=stderr))
  .stdout=stdout
  .stderr=stderr
  if (stdout=='') stdout=stderr=TRUE
  res <- base::system2(command, ..., stdout=stdout, stderr=stderr)
  if (is.character(res)) res <- gsub('[^\b](?R)?\b', '', res, perl=TRUE) # remove backspaces.
  if (.stdout=='') {
    if (.stderr==TRUE | .stdout=='') cat(res, sep='\n', file=stderr())
    if (knitr::opts_current$get('results') == 'asis') {
      cat(knitr::knit_hooks$get('output')(c(res, '\n'), knitr::opts_current$get()), file=stdout())
    } else {
      cat(res, sep='\n', file=stdout())
    }
    stat <- attr(res, 'status')
    if (is.null(stat)) stat <- 0
    attr(stat, 'errmsg') <- attr(res, 'errmsg')
    attr(stat, 'status') <- stat
    res <- stat
  } else if (.stdout==FALSE) {
    attr(res, 'status') <- res
  }
  if (is.null(attr(res, 'status'))) attr(res, 'status') <- 0
  invisible(res)
}

dbBegin <- function(conn, ...) invisible(RSQLite::dbBegin(conn, ...))
dbCommit <- function(conn, ...) invisible(RSQLite::dbCommit(conn, ...))
dbRollback <- function(conn, ...) invisible(RSQLite::dbRollback(conn, ...))

dbSendQuery <- function(conn, statement) invisible(dbClearResult(RSQLite::dbSendQuery(conn, statement)))

dbSendPreparedQuery <- function(conn, statement, df, attempts=5, ...) {
  cl <- class(conn)
  if (cl[1] == 'SQLiteConnection') {
    # do nothing
    co <- conn
    st <- statement
    df <- as.data.frame(df)
  } else if (any(c('tbl_df','tbl','data.frame') %in% cl)) {
    co <- statement
    st <- df
    df <- as.data.frame(conn)
  }
  
  # Now try to send
  i <- 0
  while (i < attempts) {
    res <- try(RSQLite::dbSendPreparedQuery(co, st, df, ...), silent = TRUE)
    if (!is(res, 'try-error')) break
    i <- i + 1
  }
  if (is(res, 'try-error')) stop(res)
  dbClearResult(res)
  invisible(res)
}

dbGetQuery <- function(conn, statement, ..., attempts=5) {
  i <- 0
  while (i < attempts) {
    res <- try(RSQLite::dbGetQuery(conn, statement, ...), silent = TRUE)
    if (!is(res, 'try-error')) break
    i <- i + 1
  }
  if (is(res, 'try-error')) stop(res)
  res
}

dbGetPreparedQuery <- function(conn, statement, df, ..., attempts=5) {
  i <- 0
  while (i < attempts) {
    res <- try(RSQLite::dbGetPreparedQuery(conn, statement, df, ...), silent = TRUE)
    if (!is(res, 'try-error')) break
    i <- i + 1
  }
  if (is(res, 'try-error')) stop(res)
  res
}
```

```{r echo=FALSE,eval=!.do.computations,results='asis'}
cat('**Will not run computations, only summaries.**\n')
```

# Preparations

## Compiling

### gfortran

```{r gfortran,eval=.do.computations}
srcdir <- file.path(BASEDIR, '..', 'Siccuracy', 'src')
files <- list.files(srcdir, pattern='*.f95')

tmp <- tempdir()
olddir <- setwd(tmp)
file.copy(file.path(srcdir, files), '.', overwrite=TRUE)
out <- file.path(TMPDIR, 'gfort.so')
system2('R', args=c('CMD SHLIB','-o', out, files))

cat('\nRe-compiling with -O3 optimizations:\n', file=stderr())
unlink('*.o')
out <- file.path(TMPDIR, 'gfortO3.so')
#system2('R', args=c('CMD SHLIB','-o', out, files), env='PKG_FCFLAGS=-O3')
# --> modules are compiled as gfortran -O3 -fpic -g -O2  -c  auxil.f95 -o auxil.o, where the last -O2 becomes the effective option.
# (https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html)
system2('R', args=c('CMD SHLIB','-o', out, files), env='PKG_FCFLAGS=\'-finline-functions -funswitch-loops -fpredictive-commoning -fgcse-after-reload -ftree-loop-distribute-patterns -ftree-slp-vectorize -fvect-cost-model -ftree-partial-pre -fpeel-loops -fipa-cp-clone\'')
```


### ifort

```{r ifort,eval=.do.computations}
tmp <- tempdir()
olddir <- setwd(tmp)
file.copy(file.path(srcdir, files), file.path(tmp, knitr:::sub_ext(files, 'f90')), overwrite=TRUE)
files <- list.files(tmp, pattern='*.f90')
out <- file.path(TMPDIR, 'ifort.so')
cat('FC=ifort\nF77=ifort\n', file='Makevars')
system2('R', args=c('CMD SHLIB','-o', out,  files), env=c('R_MAKEVARS_USER=Makevars', 'PKG_FCFLAGS=-O3'))
```

## Templates

Contents of `launcher.sh`:
```{r launcher_sh,eval=.do.computations,cache=FALSE,engine='cat',engine.opts=list(file='launcher.sh',lang='bash')}
#Rscript --no-restore prepare.R 
Rscript --no-restore test.R $TMPDIR/gfort.so &
pid=$!
./cpumemlog.sh $pid -o=cpumemlog.txt -t=1
wait
Rscript --no-restore wrapup.R gfort.so

Rscript --no-restore test.R $TMPDIR/gfortO3.so &
pid=$!
./cpumemlog.sh $pid -o=cpumemlog.txt -t=1
wait
Rscript --no-restore wrapup.R gfortO3.so

Rscript --no-restore test.R $TMPDIR/ifort.so &
pid=$!
./cpumemlog.sh $pid -o=cpumemlog.txt -t=1
wait
Rscript --no-restore wrapup.R ifort.so

```

Contents of `test.R`:

```{r test_R,eval=.do.computations,cache=FALSE,engine='cat',engine.opts=list(file='test.R',lang='R')}
load('run_options.Rdata')

args <- commandArgs(TRUE)
dyn.load(args[1])

imputation_accuracy('true.txt','impu.txt', nSNPs=m, nAnimals=n, standardized=standardized, adaptive=adaptive)
```

Contents of `wrapup.R`:

```{r wrapup_R,eval=.do.computations,cache=FALSE,engine='cat',engine.opts=list(file='wrapup.R',lang='R')}
library(RSQLite, quietly=TRUE)
#library(dplyr)
#library(tidyr)

load('run_options.Rdata')

args <- commandArgs(TRUE)
type <- args[1]

dbconn <- dbConnect(SQLite(), dbfn)
dbSendQuery(dbconn, 'PRAGMA busy_timeout=12000;')

x <- read.table('cpumemlog.txt', header=TRUE, as.is=TRUE, fill=TRUE)
x <- x[!is.na(x$VSZ),]
if (nrow(x) > 0) {
  ti <- do.call(rbind, lapply(strsplit(x$ETIME, ':', fixed=TRUE), as.integer))
  x$ETIME <- 3600*ti[,1] + 60*ti[,2] + ti[,3]
  x$type <- type
  x$jobid <- jobid
  dbSendPreparedQuery(dbconn, 'INSERT INTO results (jobid, type, ETIME, RSS, VSZ) VALUES (:jobid, :type, :ETIME, :RSS, :VSZ);', x)
}
dbDisconnect(dbconn)
```

## Functions

```{r functions,eval=.do.computations}
make.true <- function(n,m) {
  true <- matrix(sample(0:2, size = n*m, replace=TRUE), ncol=m)  # fill true with random 0, 1, or 2.
  # add non-segregating site
  i <- sample.int(m, 1)
  true[2:n,i] <- true[1,i]
  rownames(true) <- as.character(1:nrow(true))
  true
}
make.imputed <- function(true) {
  m <- ncol(true)
  n <- nrow(true)
  imputed <- true
  imputed[sample.int(n*m, floor(n*m*0.5))] <- sample(0:2, size=floor(n*m*0.5), replace=TRUE) # change half the elements
  imputed[sample.int(n*m, floor(n*m*0.1))] <- NA  # some elements are missing.
  imputed
}
write.snps <- function(x, fn, row.names=TRUE, na='9', ...) {
  write.table(x, fn, col.names=FALSE, row.names=row.names, quote=FALSE, na=na, ...)
}

run_env <- new.env()

run_env$dbSendQuery <- dbSendQuery
run_env$dbSendPreparedQuery <- dbSendPreparedQuery

run_env$imputation_accuracy <- function(truefn, imputefn, nSNPs=NULL, nAnimals=NULL, NAval=9, standardized=TRUE, adaptive=TRUE) {
  stopifnot(file.exists(truefn))
  stopifnot(file.exists(imputefn))
  
  standardized <- as.logical(standardized)
  if (is.null(nSNPs)) {
    nSNPs <- get_ncols(truefn)-1
  }
  if (is.null(nAnimals)) {
    nAnimals <- get_nlines(truefn)
  }
  m <- as.integer(nSNPs)
  n <- as.integer(nAnimals)
  
  subroutine <- ifelse(adaptive, 'imp_acc', 'imp_acc_fast')
  
  res <- .Fortran(subroutine,
                  truefn=as.character(truefn),
                  imputedfn=as.character(imputefn),
                  nSnps=m,
                  nAnimals=as.integer(nAnimals),
                  NAval=as.integer(NAval),
                  standardized=as.integer(standardized),
                  means=vector('numeric',m), sds=vector('numeric',m),  # Placeholders for return data.
                  rowcors=vector('numeric', n), matcor=numeric(1), colcors=vector('numeric',m),
                  rowID=vector('integer',n))
}

run_env$get_ncols <- function(fn) {
  on.exit(try(close(f), silent=TRUE))
  f <- gzfile(fn, 'r')
  s <- scan(f, what=character(), quiet=TRUE, nlines=1)
  close(f)
  length(s)
}

run_env$get_nlines <- function(fn, showWarning=TRUE, doError=FALSE) {
  stopifnot(file.exists(fn))
  res <- .Fortran('get_nlines', fn=as.character(fn), nlines=integer(1), stat=integer(1))
  if (res$nlines == 0 & res$stat != 0) {
    msg <- paste0('get_nlines did not read lines; stat error ', res$stat, '.')
    if (doError) { stop(msg)
    } else if (showWarning) warning(msg)
  }
  res$nlines
}
```

## Setup database for tests
```{r setup_db, cache=FALSE}
dbconn <- dbConnect(SQLite(), .results('results.sqlite'))
dbSendQuery(dbconn, 'PRAGMA busy_timeout=12000;')

dbSendQuery(dbconn, 'CREATE TABLE IF NOT EXISTS jobs (
  jobid INTEGER PRIMARY KEY,
  n INTEGER NOT NULL,
  m INTEGER NOT NULL,
  standardized TEXT NOT NULL COLLATE NOCASE,
  adaptive TEXT NOT NULL COLLATE NOCASE,
  replicate INTEGER NOT NULL DEFAULT 0,
  status INTEGER NOT NULL DEFAULT 0,
  SGE_JOBID INTEGER,
  CONSTRAINT C_UNIQUE_job UNIQUE (n, m, standardized, adaptive, replicate) ON CONFLICT IGNORE);')
dbSendQuery(dbconn, 'CREATE INDEX IF NOT EXISTS IDX_jobstatus ON jobs (status);')

dbSendQuery(dbconn, 'CREATE TABLE IF NOT EXISTS results (
  jobid INTEGER NOT NULL,
  type TEXT NOT NULL COLLATE NOCASE,
  ETIME INTEGER NOT NULL,
  RSS INTEGER NOT NULL,
  VSZ INTEGER NOT NULL,
  CONSTRAINT C_UNIQUE_result UNIQUE (jobid, type, ETIME) ON CONFLICT REPLACE);')
dbSendQuery(dbconn, 'CREATE INDEX IF NOT EXISTS IDX_results_jobid ON results (jobid, type);')

dbSendQuery(dbconn, 'CREATE TRIGGER IF NOT EXISTS tr_job_reset
  AFTER UPDATE OF status ON jobs FOR EACH ROW WHEN new.status=0
  BEGIN
    DELETE FROM results WHERE new.status=0 AND jobid=new.jobid;
  END;')

dbSendQuery(dbconn, 'CREATE VIEW IF NOT EXISTS view_jobs AS
  SELECT standardized, adaptive, sum(status==0) as pending, sum(status==2) as finished from jobs group by standardized, adaptive;')
dbSendQuery(dbconn, 'CREATE VIEW IF NOT EXISTS view_results AS
  SELECT n, m, standardized, adaptive, replicate, type, ETIME, RSS, VSZ FROM jobs INNER JOIN results USING (jobid)
  WHERE jobs.status=2;')

ns <- c(100, 500, 1000, 2000, 3000, 5000, 8000, 10000, 15000, 20000)
ms <- c(1000, 2000, 5000, 7000, 8000, 10000, 12000, 15000, 20000, 25000, 30000)

expand.grid(n=ns, m=ms, standardized=c('TRUE','FALSE'), adaptive=c('TRUE','FALSE'), replicate=1:5) %>%
  dbSendPreparedQuery(dbconn, 'INSERT OR IGNORE INTO jobs (n, m, standardized, adaptive, replicate) VALUES (:n, :m, :standardized, :adaptive, :replicate);')
```

## Main iterator

```{r iterator,eval=.do.computations,cache=FALSE}

Sys.sleep(TASKID * 10)
run_env$dbfn <- .results('results.sqlite')

file.copy(file.path(BASEDIR, '../tools/cpumemlog/cpumemlog.sh'), TMPDIR, overwrite = TRUE)
system2('chmod', c('u+x', 'cpumemlog.sh','launcher.sh'))

while (TRUE) {
  dbBegin(dbconn)
  next.job <- dbGetQuery(dbconn, 'SELECT * FROM jobs WHERE status=0 LIMIT 1;')
  if (nrow(next.job) == 0) break
  next.job$sge <- SGE_JOBID
  dbSendPreparedQuery(dbconn, 'UPDATE jobs SET status=1, SGE_JOBID=:sge WHERE jobid=:jobid;', next.job)
  dbCommit(dbconn)
  with(next.job, cat('- Running', jobid, sprintf('(n = %d, m = %d)', n, m), '\n', file=stderr()))
  
  attach(next.job)
  run_env$n <- as.integer(n)
  run_env$m <- as.integer(m)
  run_env$standardized <- as.logical(standardized)
  run_env$adaptive <- as.logical(adaptive)
  run_env$jobid <- jobid
  detach(next.job)
  
  save(list=ls(envir=run_env), envir=run_env, file='run_options.Rdata')
  
  true <- make.true(run_env$n, run_env$m)
  impu <- make.imputed(true)
  
  fn1 <- 'true.txt'
  fn2 <- 'impu.txt'
  
  unlink(c(fn1,fn2))
  
  write.snps(true, fn1)
  write.snps(impu, fn2)
  
  st <- system2('./launcher.sh', stdout=FALSE)
  if (st == 0) dbSendPreparedQuery(dbconn, 'UPDATE jobs SET status=2 WHERE jobid=:jobid;', next.job)
}
try(dbRollback(dbconn), silent=TRUE)


```

# Results

Completion status, summarised on standardized and adaptive parameters.

```{r results='as.is'}
dbGetQuery(dbconn, 'SELECT * FROM view_jobs;') %>%
  kable()
```


```{r detailed_completion_status}
dbGetQuery(dbconn, 'SELECT n, standardized, adaptive, count(jobid) as total, sum(status==2) as complete, sum(status==0) as pending FROM jobs GROUP BY n, standardized, adaptive;') %>% 
  #gather(param, val, standardized, adaptive) %>% 
  mutate(missing=total-(complete+pending)) %>%
  gather(stat, val, complete, pending, missing) %>%
  ggplot(aes(x=as.factor(n), y=val, fill=stat)) + geom_bar(stat='identity') +
  facet_wrap( ~standardized + adaptive, ncol=2, labeller=label_both) + coord_flip() +
  scale_fill_manual(values=structure(scales::hue_pal()(3), .Names=c('missing','complete','pending'))) + 
  labs(title='Detailed completion status', y='Count', x='Animals (`n`)', colour='Status')
```

## Explorative results

```{r}
p <- dbGetQuery(dbconn, 'SELECT * FROM view_results;') %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% summarise_each(funs(max), ETIME, RSS, VSZ) %>%
  gather(stat, val, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=n, y=val, colour=type)) + geom_line() +
  facet_grid(adaptive + stat ~ standardized, labeller=label_both, scales='free_y')
p
p %+% aes(x=m)
  
```

```{r}
dbGetQuery(dbconn, 'SELECT * FROM view_results;') %>% 
  group_by(n, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% 
  group_by(n, standardized, adaptive, type) %>% summarise_each(funs(mean), m, ETIME, RSS, VSZ) %>% 
  gather(param, val, m, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=n, y=val, colour=type)) + geom_line() + 
  facet_grid(adaptive+param ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both))

```

```{r}
dbGetQuery(dbconn, 'SELECT * FROM view_results;') %>% 
  group_by(m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% 
  group_by(m, standardized, adaptive, type) %>% summarise_each(funs(mean), n, ETIME, RSS, VSZ) %>% 
  gather(param, val, n, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=m, y=val, colour=type)) + geom_line() + 
  facet_grid(adaptive+param ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both))

```

How many time series do we have of each replicate?
```{r}
dbGetQuery(dbconn, 'SELECT DISTINCT n, m, standardized, adaptive, replicate, type FROM view_results;') %>% 
  group_by(n, m) %>% summarise(types=length(unique(type)), no=n()) %>% ungroup %>%
  filter(types == 3) %>% mutate(max(no)) %>% arrange(desc(n), desc(m))
view <- data.frame(n=20000, m=20000)
```

```{r}
dbGetPreparedQuery(dbconn, 'SELECT * FROM view_results WHERE n=:n;', view) %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME)  %>%# gets last observation; latter two do not decrease.
  gather(stat, val, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=m, y=val, colour=type)) + stat_summary(fun.y=mean, geom='line') +  geom_point(pch=1) +
  facet_grid(adaptive+stat ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both)) +
  labs(title=paste(view$n, 'individuals'), x='# Markers', colour='Compiler') + theme(axis.title.y=element_blank())
```

```{r}
dbGetPreparedQuery(dbconn, 'SELECT * FROM view_results WHERE m=:m;', view) %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME)  %>%# gets last observation; latter two do not decrease.
  gather(stat, val, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=n, y=val, colour=type)) + stat_summary(fun.y=mean, geom='line') +  geom_point(pch=1) +
  facet_grid(adaptive+stat ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both)) +
  labs(title=paste(view$m, 'markers'), x='# Individuals', colour='Compiler') + theme(axis.title.y=element_blank())
```


# Code preamble

The following code block is executed in the very beginning of the script.
```{r setup,eval=FALSE}
```
