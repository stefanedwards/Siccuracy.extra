---
title: "Test run time and memory usage of imputation accuracy"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: "Stefan McKinnon Hoj-Edwards"
output: 
  html_document: 
    theme: spacelab
    toc: yes
    toc_float: true
---


This vignette is for testing memory and running time of Siccuracy's `imputation_accuracy` function using native `gfortran` compiled `.so` and `ifort` compiled `.so`.
It repeats the previous memory test as it runs `imputation_accuracy` under both
`fast=TRUE` and `fast=FALSE`. When `fast=TRUE`, neither matrix is stored in memory, whereas when `fast=FALSE`,
the true genotype file is stored in memory for matching row IDs.

Further more, using `standardization=TRUE` forces the methods to read the true genotype file *twice* in order to calculate means and standard deviations of true genotypes.

To acomplish this, we do *not* load the `Siccuracy`-package.

In order to estimate memory usage, the calculation is launched in a subprocess, while using Gregor Gorjanc's [cpumemlog](https://github.com/gregorgorjanc/cpumemlog) to monitor the usage of the subprocess.

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(RSQLite)
library(cowplot)
library(grid)
library(gridExtra)

BASEDIR <- getwd()
TMPDIR <- Sys.getenv('TMPDIR',tempdir())
TASKID <- Sys.getenv('SGE_TASK_ID', 'undefined')
suppressWarnings(TASKID <- as.integer(TASKID))
if (is.na(TASKID)) TASKID <- 0
SGE_JOBID <- Sys.getenv('JOB_ID', NA)


.dot <- function(x) {
  f <- function(..., relative=FALSE) {
    if (relative) {
      file.path(file.path(x, .Platform$file.sep, c(...)))
    } else {
      file.path(BASEDIR, x,  c(...))
    }
  } 
  null <- dir.create(f(''), FALSE, TRUE)
  return(f)
}
.cached <- .dot('cache_gfortran_vs_ifort')
.results <- .dot('gfortran_vs_ifort_results')

.do.computations = .Platform$OS.type != 'windows'

knitr::opts_knit$set(root.dir=TMPDIR)
knitr::opts_chunk$set(echo = TRUE, cache=FALSE, autodep=TRUE, 
                      cache.path = .cached('/'), 
                      fig.path = .results('', relative=TRUE), 
                      fig.width=10, fig.height=7)

system2 <- function(command, ..., stdout='', stderr='') {
  if (is.null(knitr::opts_current$get('results'))) 
    return(base::system2(command, ..., stdout=stdout, stderr=stderr))
  .stdout=stdout
  .stderr=stderr
  if (stdout=='') stdout=stderr=TRUE
  res <- base::system2(command, ..., stdout=stdout, stderr=stderr)
  if (is.character(res)) res <- gsub('[^\b](?R)?\b', '', res, perl=TRUE) # remove backspaces.
  if (.stdout=='') {
    if (.stderr==TRUE | .stdout=='') cat(res, sep='\n', file=stderr())
    if (knitr::opts_current$get('results') == 'asis') {
      cat(knitr::knit_hooks$get('output')(c(res, '\n'), knitr::opts_current$get()), file=stdout())
    } else {
      cat(res, sep='\n', file=stdout())
    }
    stat <- attr(res, 'status')
    if (is.null(stat)) stat <- 0
    attr(stat, 'errmsg') <- attr(res, 'errmsg')
    attr(stat, 'status') <- stat
    res <- stat
  } else if (.stdout==FALSE) {
    attr(res, 'status') <- res
  }
  if (is.null(attr(res, 'status'))) attr(res, 'status') <- 0
  invisible(res)
}

dbBegin <- function(conn, ...) invisible(RSQLite::dbBegin(conn, ...))
dbCommit <- function(conn, ...) invisible(RSQLite::dbCommit(conn, ...))
dbRollback <- function(conn, ...) invisible(RSQLite::dbRollback(conn, ...))

dbSendQuery <- function(conn, statement) invisible(dbClearResult(RSQLite::dbSendQuery(conn, statement)))

dbSendPreparedQuery <- function(conn, statement, df, attempts=5, ...) {
  cl <- class(conn)
  if (cl[1] == 'SQLiteConnection') {
    # do nothing
    co <- conn
    st <- statement
    df <- as.data.frame(df)
  } else if (any(c('tbl_df','tbl','data.frame') %in% cl)) {
    co <- statement
    st <- df
    df <- as.data.frame(conn)
  }
  
  # Now try to send
  i <- 0
  while (i < attempts) {
    res <- try(RSQLite::dbSendPreparedQuery(co, st, df, ...), silent = TRUE)
    if (!is(res, 'try-error')) break
    i <- i + 1
  }
  if (is(res, 'try-error')) stop(res)
  dbClearResult(res)
  invisible(res)
}

dbGetQuery <- function(conn, statement, ..., attempts=5) {
  i <- 0
  while (i < attempts) {
    res <- try(RSQLite::dbGetQuery(conn, statement, ...), silent = TRUE)
    if (!is(res, 'try-error')) break
    i <- i + 1
  }
  if (is(res, 'try-error')) stop(res)
  res
}

dbGetPreparedQuery <- function(conn, statement, df, ..., attempts=5) {
  i <- 0
  while (i < attempts) {
    res <- try(RSQLite::dbGetPreparedQuery(conn, statement, df, ...), silent = TRUE)
    if (!is(res, 'try-error')) break
    i <- i + 1
  }
  if (is(res, 'try-error')) stop(res)
  res
}

g_legend<-function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
```

```{r echo=FALSE,eval=!.do.computations,results='asis'}
cat('**Will not run computations, only summaries.**\n')
```

# Preparations

## Compiling

### gfortran

```{r gfortran,eval=.do.computations}
srcdir <- file.path(BASEDIR, '..', 'Siccuracy', 'src')
files <- list.files(srcdir, pattern='*.f95')

tmp <- tempdir()
olddir <- setwd(tmp)
file.copy(file.path(srcdir, files), '.', overwrite=TRUE)
out <- file.path(TMPDIR, 'gfort.so')
system2('R', args=c('CMD SHLIB','-o', out, files))  # native
file.size(out)


cat('\nRe-compiling with -O3 optimizations:\n', file=stderr())
unlink('*.o')
out <- file.path(TMPDIR, 'gfortO3.so')
#system2('R', args=c('CMD SHLIB','-o', out, files), env='PKG_FCFLAGS=-O3')
# --> modules are compiled as gfortran -O3 -fpic -g -O2  -c  auxil.f95 -o auxil.o, where the last -O2 becomes the effective option.
# (https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html)
# pre-compile modules with optimizations before linking.
for (f in files) {
  system2('gfortran', args=c('-O3','-fpic','-c', f, '-o', knitr:::sub_ext(f, 'o')))
}
system2('R', args=c('CMD SHLIB','-o', out, files))
file.size(out)
```


### ifort

```{r ifort,eval=.do.computations}
tmp <- tempdir()
olddir <- setwd(tmp)
file.copy(file.path(srcdir, files), file.path(tmp, knitr:::sub_ext(files, 'f90')), overwrite=TRUE)
files <- list.files(tmp, pattern='*.f90')
out <- file.path(TMPDIR, 'ifort.so')
for (f in files) {
  system2('ifort', args=c('-O3','-fpic','-c', f, '-o', knitr:::sub_ext(f, 'o')))
}
cat('FC=ifort\nF77=ifort\n', file='Makevars')
system2('R', args=c('CMD SHLIB','-o', out,  files), env=c('R_MAKEVARS_USER=Makevars'))
file.size(out)
```

## Templates

Contents of `launcher.sh`:
```{r launcher_sh,eval=.do.computations,cache=FALSE,engine='cat',engine.opts=list(file='launcher.sh',lang='bash')}
Rscript --no-restore $1 $TMPDIR/$2 &
pid=$!
./cpumemlog.sh $pid -o=cpumemlog.txt -t=1
wait
Rscript --no-restore wrapup.R $2

```

Contents of `test.R`:

```{r test_R,eval=.do.computations,cache=FALSE,engine='cat',engine.opts=list(file='test.R',lang='R')}
load('run_options.Rdata')

args <- commandArgs(TRUE)
dyn.load(args[1])

imputation_accuracy('true.txt','impu.txt', ncol=m, nlines=n, standardized=standardized, adaptive=adaptive)
```

Contents of `wrapup.R`:

```{r wrapup_R,eval=.do.computations,cache=FALSE,engine='cat',engine.opts=list(file='wrapup.R',lang='R')}
library(RSQLite, quietly=TRUE)

load('run_options.Rdata')

args <- commandArgs(TRUE)
type <- args[1]

dbconn <- dbConnect(SQLite(), dbfn)
dbSendQuery(dbconn, 'PRAGMA busy_timeout=12000;')

x <- read.table('cpumemlog.txt', header=TRUE, as.is=TRUE, fill=TRUE)
x <- x[!is.na(x$VSZ),]
if (nrow(x) > 0) {
  ti <- do.call(rbind, lapply(strsplit(x$ETIME, ':', fixed=TRUE), as.integer))
  x$ETIME <- 3600*ti[,1] + 60*ti[,2] + ti[,3]
  x$type <- type
  x$jobid <- jobid
  dbSendPreparedQuery(dbconn, 'INSERT INTO results (jobid, type, ETIME, RSS, VSZ) VALUES (:jobid, :type, :ETIME, :RSS, :VSZ);', x)
}
dbDisconnect(dbconn)
```

Contents of `R.R`:
Estimating natively in R.

```{r R_R,eval=.do.computations,cache=FALSE,engine='cat',engine.opts=list(file='R.R',lang='R')}
load('run_options.Rdata')

true <- read.snps('true.txt', nlines=n, ncols=m+1, what=integer(), na=9)
impu <- read.snps('impu.txt', nlines=n, ncols=m+1, what=numeric(), na=9)

if (standardized) {
  m <- apply(true, 2, mean, na.rm=TRUE)
  v <- apply(true, 2, sd, na.rm=TRUE)
  true <- scale(true, m, v)
  impu <- scale(impu, m, v)
}

mat1 <- cor(as.vector(true), as.vector(impu), use = 'complete.obs')
row1 <- sapply(1:nrow(true), function(i) cor(true[i,], impu[i,], use='na.or.complete'))
col1 <- sapply(1:ncol(true), function(i) cor(true[,i], impu[,i], use='na.or.complete'))

```

## Functions

```{r functions,eval=.do.computations}
make.true <- function(n,m) {
  true <- matrix(sample(0:2, size = n*m, replace=TRUE), ncol=m)  # fill true with random 0, 1, or 2.
  # add non-segregating site
  i <- sample.int(m, 1)
  true[2:n,i] <- true[1,i]
  rownames(true) <- as.character(1:nrow(true))
  true
}
make.imputed <- function(true) {
  m <- ncol(true)
  n <- nrow(true)
  imputed <- true
  imputed[sample.int(n*m, floor(n*m*0.5))] <- sample(0:2, size=floor(n*m*0.5), replace=TRUE) # change half the elements
  imputed[sample.int(n*m, floor(n*m*0.1))] <- NA  # some elements are missing.
  imputed
}
write.snps <- function(x, fn, row.names=TRUE, na='9', ...) {
  write.table(x, fn, col.names=FALSE, row.names=row.names, quote=FALSE, na=na, ...)
}

run_env <- new.env()

run_env$dbSendQuery <- dbSendQuery
run_env$dbSendPreparedQuery <- dbSendPreparedQuery

run_env$imputation_accuracy <- function(truefn, imputefn, ncol=NULL, nlines=NULL, na=9, standardized=TRUE, adaptive=TRUE, center=NULL, scale=NULL, p=NULL) {
  stopifnot(file.exists(truefn))
  stopifnot(file.exists(imputefn))

  standardized <- as.logical(standardized)
  if (is.null(ncol)) ncol <- get_ncols(truefn)-1
  if (is.null(nlines)) nlines <- get_nlines(truefn)

  m <- as.integer(ncol)
  n <- as.integer(nlines)

  usermeans <-  (!is.null(p) | !is.null(center) | !is.null(scale))
  if (!is.null(p)) {
    stopifnot(length(p)==m)
    center <- 2*p
    scale <- 2*p*(1-p)
  }
  if (is.null(center)) center=numeric(m)
  if (is.null(scale)) {scale=numeric(m);scale[] <- 1}
  if (usermeans) standardized=TRUE


  subroutine <- ifelse(adaptive, 'imp_acc', 'imp_acc_fast')

  res <- .Fortran(subroutine,
                  truefn=as.character(truefn),
                  imputedfn=as.character(imputefn),
                  nSnps=m,
                  nAnimals=as.integer(nlines),
                  NAval=as.integer(na),
                  standardized=as.integer(standardized),
                  means=as.numeric(center), sds=as.numeric(scale),  # Placeholders for return data.
                  usermeans=as.integer(usermeans),
                  rowcors=vector('numeric', n), matcor=numeric(1), colcors=vector('numeric',m),
                  rowID=vector('integer',n))

}

run_env$get_ncols <- function(fn) {
  on.exit(try(close(f), silent=TRUE))
  f <- gzfile(fn, 'r')
  s <- scan(f, what=character(), quiet=TRUE, nlines=1)
  close(f)
  length(s)
}

run_env$get_nlines <- function(fn, showWarning=TRUE, doError=FALSE) {
  stopifnot(file.exists(fn))
  res <- .Fortran('get_nlines', fn=as.character(fn), nlines=integer(1), stat=integer(1))
  if (res$nlines == 0 & res$stat != 0) {
    msg <- paste0('get_nlines did not read lines; stat error ', res$stat, '.')
    if (doError) { stop(msg)
    } else if (showWarning) warning(msg)
  }
  res$nlines
}

run_env$read.snps <- function(file, nlines=0, ncols=NULL, na=NA, what=integer(), extractIDs=TRUE, quiet=TRUE, ...) {
  if (is.null(ncols) & is.character(file)) {
    ncols <- get_ncols(file)
  }
  if (is.null(ncols)) stop('Cannot automagically detect number of columns to read as input file is a connection, not a character.')

  M <- matrix(scan(file, nlines=nlines, what=what, quiet=quiet, ...), ncol=ncols, byrow=TRUE)
  if (nrow(M) == 0) return(M)
  if (extractIDs) {
    rownames(M) <- M[,1]
    M <- M[,-1]
  }
  if (!is.na(na)) M[M==na] <- NA
  M
}

```

## Setup database for tests
```{r setup_db, cache=FALSE}
dbconn <- dbConnect(SQLite(), .results('results.sqlite'))
dbSendQuery(dbconn, 'PRAGMA busy_timeout=12000;')

dbSendQuery(dbconn, 'CREATE TABLE IF NOT EXISTS jobs (
  jobid INTEGER PRIMARY KEY,
  n INTEGER NOT NULL,
  m INTEGER NOT NULL,
  standardized TEXT NOT NULL COLLATE NOCASE,
  adaptive TEXT NOT NULL COLLATE NOCASE,
  replicate INTEGER NOT NULL DEFAULT 0,
  status INTEGER NOT NULL DEFAULT 0,
  SGE_JOBID INTEGER,
  CONSTRAINT C_UNIQUE_job UNIQUE (n, m, standardized, adaptive, replicate) ON CONFLICT IGNORE);')
dbSendQuery(dbconn, 'CREATE INDEX IF NOT EXISTS IDX_jobstatus ON jobs (status);')

dbSendQuery(dbconn, 'CREATE TABLE IF NOT EXISTS results (
  jobid INTEGER NOT NULL,
  type TEXT NOT NULL COLLATE NOCASE,
  ETIME INTEGER NOT NULL,
  RSS INTEGER NOT NULL,
  VSZ INTEGER NOT NULL,
  CONSTRAINT C_UNIQUE_result UNIQUE (jobid, type, ETIME) ON CONFLICT REPLACE);')
dbSendQuery(dbconn, 'CREATE INDEX IF NOT EXISTS IDX_results_jobid ON results (jobid, type);')

dbSendQuery(dbconn, 'CREATE TRIGGER IF NOT EXISTS tr_job_reset
  AFTER UPDATE OF status ON jobs FOR EACH ROW WHEN new.status=0
  BEGIN
    DELETE FROM results WHERE new.status=0 AND jobid=new.jobid;
  END;')

dbSendQuery(dbconn, 'CREATE VIEW IF NOT EXISTS view_jobs AS
  SELECT standardized, adaptive, sum(status==0) as pending, sum(status==2) as finished from jobs group by standardized, adaptive;')
dbSendQuery(dbconn, 'CREATE VIEW IF NOT EXISTS view_results AS
  SELECT n, m, standardized, adaptive, replicate, type, ETIME, RSS, VSZ FROM jobs INNER JOIN results USING (jobid)
  WHERE jobs.status=2;')

ns <- c(100, 500, 1000, 2000, 3000, 5000, 8000, 10000, 15000, 20000)
ms <- c(1000, 2000, 5000, 7000, 8000, 10000, 12000, 15000, 20000, 25000, 30000)

expand.grid(n=ns, m=ms, standardized=c('TRUE','FALSE'), adaptive=c('TRUE','FALSE'), replicate=1:5) %>%
  dbSendPreparedQuery(dbconn, 'INSERT OR IGNORE INTO jobs (n, m, standardized, adaptive, replicate) VALUES (:n, :m, :standardized, :adaptive, :replicate);')
```

## Main iterator

```{r iterator,eval=.do.computations,cache=FALSE}

Sys.sleep(TASKID * 10)
run_env$dbfn <- .results('results.sqlite')

null <- file.copy(file.path(BASEDIR, 'tools/cpumemlog/cpumemlog.sh'), TMPDIR, overwrite = TRUE)
stopifnot(null)
system2('chmod', c('u+x', 'cpumemlog.sh','launcher.sh'))

while (TRUE) {
  dbBegin(dbconn)
  next.job <- dbGetQuery(dbconn, 'SELECT * FROM jobs WHERE status=0 ORDER BY replicate ASC LIMIT 1;')
  if (nrow(next.job) == 0) break
  next.job$sge <- SGE_JOBID
  dbSendPreparedQuery(dbconn, 'UPDATE jobs SET status=1, SGE_JOBID=:sge WHERE jobid=:jobid;', next.job)
  dbCommit(dbconn)
  with(next.job, cat('- Running', jobid, sprintf('(n = %d, m = %d)', n, m), '\n', file=stderr()))
  
  attach(next.job)
  run_env$n <- as.integer(n)
  run_env$m <- as.integer(m)
  run_env$standardized <- as.logical(standardized)
  run_env$adaptive <- as.logical(adaptive)
  run_env$jobid <- jobid
  detach(next.job)
  
  save(list=ls(envir=run_env), envir=run_env, file='run_options.Rdata')
  
  true <- make.true(run_env$n, run_env$m)
  impu <- make.imputed(true)
  
  fn1 <- 'true.txt'
  fn2 <- 'impu.txt'
  
  unlink(c(fn1,fn2))
  
  write.snps(true, fn1)
  write.snps(impu, fn2)
  
  st <- 0
  for (s in c('gfort.so','gfortO3.so','ifort.so'))  {
    st <- st + system2('./launcher.sh', args=c('test.R',s), stdout=FALSE)
  }
  st <- st + system2('./launcher.sh', args=c('R.R', 'R'), stdout=FALSE)
  
  if (st == 0) dbSendPreparedQuery(dbconn, 'UPDATE jobs SET status=2 WHERE jobid=:jobid;', next.job)
}
try(dbRollback(dbconn), silent=TRUE)


```

# Results

Completion status, summarised on standardized and adaptive parameters.

```{r results='as.is'}
dbGetQuery(dbconn, 'SELECT * FROM view_jobs;') %>%
  kable()
```


```{r detailed_completion_status}
dbGetQuery(dbconn, 'SELECT n, standardized, adaptive, count(jobid) as total, sum(status==2) as complete, sum(status==0) as pending FROM jobs GROUP BY n, standardized, adaptive;') %>% 
  #gather(param, val, standardized, adaptive) %>% 
  mutate(missing=total-(complete+pending)) %>%
  gather(stat, val, complete, pending, missing) %>%
  ggplot(aes(x=as.factor(n), y=val, fill=stat)) + geom_bar(stat='identity') +
  facet_wrap( ~standardized + adaptive, ncol=2, labeller=label_both) + coord_flip() +
  scale_fill_manual(values=structure(scales::hue_pal(c(0,360)+180)(3), .Names=c('complete','missing','pending'))) + 
  labs(title='Detailed completion status', y='Count', x='Animals (`n`)', colour='Status')
```

```{r deconstruct_results_by_type}
dbGetQuery(dbconn, 'SELECT n, m, standardized, adaptive, type, ETIME FROM view_results;') %>%
  group_by(n, m, standardized, adaptive, type) %>% top_n(1, ETIME) %>% summarise(no=n()) %>% 
  ggplot(aes(x=1, y=no, fill=type)) + geom_bar(stat='identity', position='dodge') + 
  facet_grid(adaptive + n ~ standardized + m) +
  coord_polar() + theme(axis.title=element_blank(), axis.text=element_blank(), axis.ticks=element_blank())
```

## Explorative results

```{r}
dbGetQuery(dbconn, 'SELECT * FROM view_results;') %>% 
  group_by(n, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% 
  group_by(n, standardized, adaptive, type) %>% summarise_each(funs(mean), m, ETIME, RSS, VSZ) %>% 
  gather(param, val, m, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=n, y=val, colour=type)) + geom_line() + 
  facet_grid(adaptive+param ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both))

```

```{r}
dbGetQuery(dbconn, 'SELECT * FROM view_results;') %>% 
  group_by(m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% 
  group_by(m, standardized, adaptive, type) %>% summarise_each(funs(mean), n, ETIME, RSS, VSZ) %>% 
  gather(param, val, n, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=m, y=val, colour=type)) + geom_line() + 
  facet_grid(adaptive+param ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both))

```

```{r}
dbGetQuery(dbconn, 'SELECT * FROM view_results;') %>%
  gather(stat, val, RSS, VSZ) %>%
  ggplot(aes(x=ETIME, y=val, colour=type, group=interaction(type, replicate, n, m))) + geom_line() +
  facet_grid(adaptive+stat ~ standardized, scales='free_y')
```

```{r}
dbGetQuery(dbconn, 'SELECT n, m, standardized, adaptive, replicate, type, ETIME FROM view_results;') %>%
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>%
  ggplot(aes(x=n*m, y=ETIME, colour=type)) + geom_point() +
  facet_grid(standardized ~ adaptive, labeller=label_both)
```

```{r}
dbGetQuery(dbconn, 'SELECT n, m, standardized, adaptive, replicate, type, RSS FROM view_results;') %>%
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, RSS) %>%
  ggplot(aes(x=n*m, y=RSS, colour=type)) + geom_point(pch=1) +
  facet_grid(adaptive ~ standardized, scales='free_y', labeller=label_both)
```

```{r}
dbGetQuery(dbconn, 'SELECT n, m, standardized, adaptive, replicate, type, RSS FROM view_results;') %>%
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, RSS) %>%
  ggplot(aes(x=n, y=RSS, colour=type)) + geom_point() +
  facet_grid(adaptive ~ standardized, scales='free_y', labeller=label_both)
```

```{r}
dbGetQuery(dbconn, 'SELECT n, m, standardized, adaptive, replicate, type, RSS FROM view_results;') %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, RSS) %>%
  ggplot(aes(x=m, y=RSS, colour=type)) + geom_point(pch=1) +
  facet_grid(adaptive ~ standardized, scales='free_y', labeller=label_both)
```

How many time series do we have of each replicate?
```{r}
dbGetQuery(dbconn, 'SELECT DISTINCT n, m, standardized, adaptive, replicate, type FROM view_results;') %>% 
  group_by(n, m) %>% summarise(types=length(unique(type)), no=n()) %>% ungroup %>%
  filter(types == 4) %>% mutate(max(no)) %>% arrange(desc(n), desc(m))
view <- data.frame(n=20000, m=20000)
```

```{r}
dbGetPreparedQuery(dbconn, 'SELECT * FROM view_results WHERE n=:n;', view) %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME)  %>%# gets last observation; latter two do not decrease.
  gather(stat, val, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=m, y=val, colour=type)) + stat_summary(fun.y=mean, geom='line') +  geom_point(pch=1) +
  facet_grid(adaptive+stat ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both)) +
  labs(title=paste(view$n, 'individuals'), x='# Markers', colour='Compiler') + theme(axis.title.y=element_blank())
```

```{r}
dbGetPreparedQuery(dbconn, 'SELECT * FROM view_results WHERE m=:m;', view) %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME)  %>%# gets last observation; latter two do not decrease.
  gather(stat, val, ETIME, RSS, VSZ) %>%
  ggplot(aes(x=n, y=val, colour=type)) + stat_summary(fun.y=mean, geom='line') +  geom_point(pch=1) +
  facet_grid(adaptive+stat ~ standardized, scales='free_y', labeller=labeller(standardized=label_both, adaptive=label_both)) +
  labs(title=paste(view$m, 'markers'), x='# Individuals', colour='Compiler') + theme(axis.title.y=element_blank())
```

# Publication

```{r}
pub.theme <- theme_bw() + theme(panel.border=element_blank(), axis.line.x=element_line(), axis.line.y=element_line(), legend.key=element_blank(), strip.background=element_blank(), strip.text=element_text(size=rel(1)))

# p$coordinates$render_axis_h
h_axis_open_right <- function(p) {
  p$coordinates$render_axis_h <-  function(scale_details, theme) {
    abg <- ggplot2:::guide_axis(scale_details$x.major, scale_details$x.labels, 
          "bottom", theme)
    abg$children[[1]] <- ggplot2:::element_render(theme, "axis.line.x", c(min(scale_details$x.major), 1), c(1, 1), id.lengths = 2)
    abg
  }
  p
}
v_axis_open_top <- function(p) {
  p$coordinates$render_axis_v <- function(scale_details, theme) {
    abg <- ggplot2:::guide_axis(scale_details$y.major, scale_details$y.labels, 
          "left", theme)
    abg$children[[1]] <- ggplot2:::element_render(theme, "axis.line.x", c(1,1), c(min(scale_details$y.major, 0.05), 1), id.lengths = 2)
    abg
  }
  p
}
minutes <- function(x) {x/60}
thousands <- function(x) prettyNum(x, big.mark=',')
tp.br <- c('R','gfort.so','gfortO3.so','ifort.so')
tp.lbl <- c('R','gfort','gfort (O3)','ifort (O3)')
type.col.scale <- scale_colour_hue('Compiler:', breaks=tp.br, labels=tp.lbl)
type.sh.scale <- scale_shape('Compiler:', breaks=tp.br, labels=tp.lbl, solid=TRUE)
fct.labeller <- labeller(adaptive=c('TRUE'='Adaptive','FALSE'='Non-adaptive'), standardized=c('TRUE'='Standardized','FALSE'='Not standardized'))
se <- function(x, ...) {sd(x, ...)/sum(!is.na(x))}
nn <- function(x) {sum(!is.na(x))}
```

How many time series do we have of each replicate?
```{r}
dbGetQuery(dbconn, 'SELECT DISTINCT n, m, standardized, adaptive, replicate, type FROM view_results;') %>% 
  group_by(n, m) %>% summarise(types=length(unique(type)), no=n()) %>% ungroup %>%
  filter(types == 4) %>% mutate(max(no)) %>% arrange(desc(n), desc(m))
view <- data.frame(n=20000, m=20000)
```

```{r}
dbGetQuery(dbconn, 'SELECT n, m, adaptive, standardized, replicate, type, ETIME, RSS, VSZ FROM view_results WHERE m=30000 AND n=20000;') %>%
  group_by(n, m, adaptive, standardized, replicate, type) %>% top_n(1, ETIME) %>% 
  mutate(RSS=RSS/1024) %>%
  group_by(adaptive, standardized, type) %>% summarise_each(funs(mean, se, nn), ETIME, RSS)
```

## Speed of Fortran compiled in non-standardized

```{r}
pm <- dbGetPreparedQuery(dbconn, 'SELECT n, m, adaptive, replicate, type, ETIME, RSS, VSZ FROM view_results WHERE standardized="FALSE" and adaptive="FALSE" AND m=:m;', view) %>% 
  group_by(n, m, adaptive, replicate, type) %>% top_n(1, ETIME) %>% # gets last observation;
  mutate(ETIME=ETIME/60) %>%
  ggplot(aes(x=n, y=ETIME, colour=type, shape=type)) + stat_summary(fun.y=mean, geom='line') + geom_point() + 
  scale_y_continuous('Elapsed time (min)') + 
  scale_x_continuous(labels=thousands) + type.col.scale + type.sh.scale +
  labs(x='Individuals (`n`)', title=paste0(thousands(view$m),' SNPs')) +
  coord_cartesian(ylim=c(0,5)) + 
  pub.theme + theme(legend.position='hidden')
pn <- dbGetPreparedQuery(dbconn, 'SELECT n, m, adaptive, replicate, type, ETIME, RSS, VSZ FROM view_results WHERE standardized="FALSE" and adaptive="FALSE" AND n=:n;', view) %>% 
  group_by(n, m, adaptive, replicate, type) %>% top_n(1, ETIME) %>% # gets last observation;
  mutate(ETIME=ETIME/60) %>%
  ggplot(aes(x=m, y=ETIME, colour=type, shape=type)) + stat_summary(fun.y=mean, geom='line') + geom_point() + 
  scale_y_continuous('Elapsed time (min)') + scale_x_continuous(labels=thousands) + 
  labs(x='SNPs (`m`)', title=paste0(thousands(view$n),' individuals')) +
  type.col.scale + type.sh.scale +
  coord_cartesian(ylim=c(0,5)) + 
  pub.theme + theme(legend.position='hidden')
#p %>%  v_axis_open_top %>% h_axis_open_right
legend <- g_legend(pm + theme(legend.position='bottom'))
plot_grid((pm+theme(plot.title=element_text(size=rel(1)))) %>% v_axis_open_top %>% h_axis_open_right, 
          (pn+theme(plot.title=element_text(size=rel(1)), axis.text.y=element_blank(), axis.title.y=element_blank(), axis.line.y=element_blank(), axis.ticks.y=element_blank())) %>% h_axis_open_right, 
          labels='AUTO', label_size=12, align='h') %>%
  grid.arrange(top=textGrob('Time to calculate correlation', gp=ggplot2:::element_render(pub.theme, 'plot.title')$gp), bottom=legend$grobs[[1]])
```


## Speed of non-adaptive vs. adaptive

```{r}
p <- dbGetPreparedQuery(dbconn, 'SELECT n, m, adaptive, standardized, replicate, type, ETIME, RSS, VSZ FROM view_results WHERE type != "R" AND m=:m;', view) %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% # gets last observation;
  mutate(ETIME=ETIME/60) %>%
  ggplot(aes(x=n, y=ETIME, colour=type, shape=type)) + stat_summary(fun.y=mean, geom='line') + geom_point() + 
  scale_y_continuous('Elapsed time (min)') + 
  scale_x_continuous(labels=thousands) + type.col.scale + type.sh.scale +
  labs(x='Individuals (`n`)', title=paste0('Time to calculate correlation: ', thousands(view$m),' SNPs')) +
  facet_grid(standardized~adaptive, labeller=fct.labeller, switch='y', scales='free_y') +
  pub.theme + theme(legend.position='bottom')
p %>% v_axis_open_top %>% h_axis_open_right
```

## Memory usage

```{r}
p <- dbGetPreparedQuery(dbconn, 'SELECT n, m, adaptive, standardized, replicate, type, ETIME, RSS, VSZ FROM view_results WHERE type != "R" AND m=:m AND RSS > 0;', view) %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% # gets last observation;
  ggplot(aes(x=n, y=RSS/1024, colour=type, shape=type)) + stat_summary(fun.y=mean, geom='line') + geom_point() + 
  scale_x_continuous(labels=thousands) + type.col.scale + type.sh.scale +
  labs(x='Individuals (`n`)', y='Resident Set Size (MB)', title=paste0('Memory usage for calculating correlation: ', thousands(view$m),' SNPs')) +
  facet_grid(adaptive~standardized, labeller=fct.labeller, switch='y', scales='free_y') +
  pub.theme + theme(legend.position='bottom')
p %>% v_axis_open_top %>% h_axis_open_right
```



```{r}
p <- dbGetPreparedQuery(dbconn, 'SELECT n, m, adaptive, standardized, replicate, type, ETIME, RSS, VSZ FROM view_results WHERE type != "R" AND n=:n AND RSS > 0;', view) %>% 
  group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% # gets last observation;
  #filter(adaptive=="TRUE", standardized=="FALSE", type=="ifort.so", m==10000)
  ggplot(aes(x=m, y=RSS/1024, colour=type)) + stat_summary(fun.y=mean, geom='line') + geom_point() + 
  scale_x_continuous(labels=thousands) + type.scale +
  labs(x='SNPs (`m`)', y='Resident Set Size (MB)', title=paste0('Memory usage for calculating correlation: ', thousands(view$n),' individuals')) +
  facet_grid(adaptive~standardized, labeller=fct.labeller, switch='y', scales='free_y') +
  pub.theme + theme(legend.position='bottom')
p %>% v_axis_open_top %>% h_axis_open_right
```

```{r eval=FALSE, include=FALSE}
# Some non-standardized, adaptive ifort compiled results have very low memory usages
dbGetQuery(dbconn, 'SELECT * FROM view_results WHERE n=20000 AND adaptive="TRUE" and RSS < 1000*1024 AND type="ifort.so";') %>%
group_by(n, m, standardized, adaptive, replicate, type) %>% top_n(1, ETIME) %>% View
```

# Code preamble

The following code block is executed in the very beginning of the script.
```{r setup,eval=FALSE}
```
